{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/muciyu/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1938.txt 1940.txt 348.txt  381.txt  385.txt  485.txt\r\n",
      "1939.txt 347.txt  349.txt  384.txt  483.txt  486.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls data/train_test/files/text*subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Text + Lower all Characters + Remove Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'data/train_test/files/text subset/'\n",
    "file = file_path +'384.txt'\n",
    "shakes = open(file, 'r')\n",
    "text = shakes.read().replace(\"-\\n\", \"\").replace(\"\\n\", \" \")\n",
    "lowers = text.lower()\n",
    "\n",
    "no_num = re.sub(r'\\d+', '', lowers)\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "text_tokens = tokenizer.tokenize(no_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('vitamin', 106), ('folate', 76), ('acid', 53), ('folic', 46), ('serum', 46), ('status', 38), ('deficiency', 31), ('high', 31), ('fortification', 29), ('anemia', 25), ('cognitive', 23), ('low', 21), ('normal', 21), ('intake', 19), ('homocysteine', 18), ('pmol', 17), ('interaction', 16), ('concentrations', 16), ('impairment', 15), ('nmol', 15), ('clin', 14), ('nutr', 14), ('subjects', 13), ('data', 12), ('nhanes', 12), ('among', 12), ('selhub', 11), ('mean', 11), ('food', 11), ('health', 10), ('nutrition', 10), ('mma', 10), ('effect', 10), ('relation', 9), ('pernicious', 9), ('plasma', 9), ('prevalence', 9), ('participants', 9), ('survey', 8), ('jacques', 7), ('rosenberg', 7), ('category', 7), ('iii', 7), ('suppl', 7), ('thf', 7), ('furthermore', 7), ('results', 7), ('although', 7), ('elderly', 6), ('national', 6), ('methylmalonic', 6), ('function', 6), ('circulating', 6), ('associated', 6), ('blood', 6), ('also', 6), ('liver', 6), ('extract', 6), ('factor', 6), ('neurologic', 6), ('examination', 5), ('compared', 5), ('specifically', 5), ('used', 5), ('methyl', 5), ('group', 5), ('consequences', 5), ('deficient', 5), ('products', 5), ('higher', 5), ('study', 5), ('letter', 5), ('morris', 4), ('reports', 4), ('however', 4), ('hypothesis', 4), ('impaired', 4), ('activity', 4), ('methionine', 4), ('form', 4), ('may', 4), ('synthesis', 4), ('adverse', 4), ('nervous', 4), ('system', 4), ('illness', 4), ('combined', 4), ('research', 4), ('mandatory', 4), ('flour', 4), ('available', 4), ('increased', 4), ('yeast', 4), ('administration', 4), ('united', 4), ('states', 4), ('neural', 4), ('tube', 4), ('defects', 4), ('upper', 4)]\n"
     ]
    }
   ],
   "source": [
    "manual_stopwords = ['www', 'org', 'vol']\n",
    "filtered = [w for w in text_tokens if not w in stopwords.words('english')+manual_stopwords]\n",
    "filtered2 = [c for c in filtered if len(c) > 2]\n",
    "count = Counter(filtered2)\n",
    "print (count.most_common(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('vitamin', 108), ('folat', 77), ('acid', 53), ('folic', 46), ('serum', 46), ('statu', 38), ('defici', 37), ('high', 31), ('fortif', 29), ('anemia', 27), ('cognit', 26), ('normal', 22), ('intak', 21), ('low', 21), ('impair', 19), ('interact', 18), ('concentr', 18), ('homocystein', 18), ('pmol', 17), ('nmol', 15), ('increas', 14), ('clin', 14), ('nutr', 14), ('effect', 14), ('subject', 13), ('relat', 12), ('data', 12), ('nhane', 12), ('among', 12), ('food', 12), ('selhub', 11), ('use', 11), ('nutrit', 11), ('mean', 11), ('particip', 11), ('result', 11), ('consequ', 11), ('health', 10), ('mma', 10), ('pernici', 9), ('plasma', 9), ('preval', 9), ('associ', 9), ('supplement', 9), ('suggest', 8), ('survey', 8), ('categori', 8), ('function', 8), ('factor', 8), ('jacqu', 7), ('rosenberg', 7), ('report', 7), ('iii', 7), ('suppl', 7), ('thf', 7), ('furthermor', 7), ('studi', 7), ('although', 7), ('neurolog', 7), ('advers', 6), ('affect', 6), ('elderli', 6), ('observ', 6), ('nation', 6), ('methylmalon', 6), ('refer', 6), ('compar', 6), ('circul', 6), ('specif', 6), ('blood', 6), ('also', 6), ('combin', 6), ('age', 6), ('liver', 6), ('extract', 6), ('product', 6), ('senior', 6), ('indic', 5), ('examin', 5), ('respect', 5), ('show', 5), ('activ', 5), ('unit', 5), ('methyl', 5), ('group', 5), ('pathway', 5), ('author', 5), ('review', 5), ('prevent', 5), ('higher', 5), ('cereal', 5), ('grain', 5), ('anaemia', 5), ('letter', 5), ('biochem', 4), ('morri', 4), ('howev', 4), ('hypothesi', 4), ('occur', 4), ('methionin', 4)]\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import *\n",
    "\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stemmed = stem_tokens(filtered2, stemmer)\n",
    "count = Counter(stemmed)\n",
    "print (count.most_common(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting things together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('data/train_test/files/text subset/', [], ['1939.txt', '1938.txt', '349.txt', '348.txt', '381.txt', '385.txt', '347.txt', '384.txt', '1940.txt', '486.txt', '485.txt', '483.txt'])\n"
     ]
    }
   ],
   "source": [
    "def gen_token(file, manual_stopwords = ['www', 'org', 'vol'])\n",
    "    '''\n",
    "    INPUT\n",
    "    file: path of a text file\n",
    "    manual_stopwords: additional stopwords, set manually\n",
    "    \n",
    "    Output: \n",
    "    Preprocessed and tokenized text\n",
    "    '''\n",
    "    \n",
    "    #read text file\n",
    "    raw = open(file, 'r')\n",
    "    text = raw.read().replace(\"-\\n\", \"\").replace(\"\\n\", \" \")\n",
    "    \n",
    "    #lower all letters\n",
    "    lowers = text.lower()\n",
    "    \n",
    "    #remove all numbers\n",
    "    no_num = re.sub(r'\\d+', '', lowers)\n",
    "\n",
    "    #tokenize text\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    text_tokens = tokenizer.tokenize(no_num)\n",
    "    \n",
    "    #remove stopwords\n",
    "    filtered = [w for w in text_tokens if not w in stopwords.words('english')+manual_stopwords]\n",
    "    filtered2 = [c for c in filtered if len(c) > 2]\n",
    "    \n",
    "    #stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed = stem_tokens(filtered2, stemmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/train_test/files/text subset/'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
