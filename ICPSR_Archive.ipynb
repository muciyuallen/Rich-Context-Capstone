{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ICPSR Dataset Archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/tanyanabila/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual, HBox, VBox, Label\n",
    "import ipywidgets as widgets\n",
    "import pandas as pd\n",
    "import community\n",
    "import math\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "roman = re.compile(\"\"\"\n",
    "    ^                   # beginning of string\n",
    "    M{0,4}              # thousands - 0 to 4 M's\n",
    "    (CM|CD|D?C{0,3})    # hundreds - 900 (CM), 400 (CD), 0-300 (0 to 3 C's),\n",
    "                        #            or 500-800 (D, followed by 0 to 3 C's)\n",
    "    (XC|XL|L?X{0,3})    # tens - 90 (XC), 40 (XL), 0-30 (0 to 3 X's),\n",
    "                        #        or 50-80 (L, followed by 0 to 3 X's)\n",
    "    (IX|IV|V?I{0,3})    # ones - 9 (IX), 4 (IV), 0-3 (0 to 3 I's),\n",
    "                        #        or 5-8 (V, followed by 0 to 3 I's)\n",
    "    $                   # end of string\n",
    "    \"\"\" ,re.VERBOSE)\n",
    "\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "datasets = pd.read_json('data/train_test/data_sets.json')\n",
    "publications = pd.read_json('data/train_test/publications.json')\n",
    "datasets['title_lower'] = datasets['title'].str.lower()\n",
    "publications['title_lower'] = publications['title'].str.lower()\n",
    "\n",
    "def remove_roman(sent):\n",
    "    sent = word_tokenize(sent)\n",
    "    sent = [i for i in sent if not roman.search(i)]\n",
    "    return \" \".join(sent)\n",
    "def simplify_title(s):\n",
    "    s = remove_roman(s)\n",
    "    s = s.lower()\n",
    "    s = re.sub(\"[^a-zA-Z]+\", \"\", s)\n",
    "    return s\n",
    "\n",
    "def writeNX(filename, g):\n",
    "    network_json = json_graph.node_link_data(g)\n",
    "    json.dump(network_json, open(filename, 'w'), indent=2)\n",
    "def readNX(filename):\n",
    "    with open(filename) as f:\n",
    "        js_graph = json.load(f)\n",
    "    return json_graph.node_link_graph(js_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c72c9827522c47c39aa46b958c5c45c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='By', options=('Dataset Title', 'Dataset ID', 'Publication Title', â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact\n",
    "\n",
    "def showDatasets(By = ['Dataset Title', 'Dataset ID', 'Publication Title', 'Publication ID', 'DOI'], Search=\"ANES 1952\"):\n",
    "    column = {\n",
    "        'Dataset Title': 'title_lower',\n",
    "        'Dataset ID': 'data_set_id',\n",
    "        'Publication Title': 'title_lower',\n",
    "        'Publication ID': 'publication_id',\n",
    "        'DOI': 'unique_identifier'\n",
    "    }\n",
    "    slist = Search.lower().split(',')\n",
    "#     return slist\n",
    "    try:\n",
    "        if(By == 'Dataset Title'):\n",
    "            return datasets[['data_set_id', 'unique_identifier','title', 'description']].loc[datasets[column[By]].str.contains('|'.join(slist))]\n",
    "        elif (By == 'Dataset ID'):\n",
    "            tmp = datasets[['data_set_id', 'unique_identifier','title', 'description']].loc[datasets[column[By]].isin([int(n) for n in slist])]\n",
    "            sorter = pd.DataFrame([int(n) for n in slist], columns=['data_set_id'])\n",
    "            return pd.merge(sorter, tmp, how='left', on='data_set_id')\n",
    "        elif (By == 'Publication Title'):\n",
    "            return publications[['publication_id', 'unique_identifier','title']].loc[publications[column[By]].str.contains('|'.join(slist))]\n",
    "        elif (By == 'Publication ID'):\n",
    "            tmp = publications[['publication_id', 'unique_identifier','title']].loc[publications[column[By]].isin([int(n) for n in slist])]\n",
    "            sorter = pd.DataFrame([int(n) for n in slist], columns=['publication_id'])\n",
    "            return pd.merge(sorter, tmp, how='left', on='publication_id')\n",
    "        elif (By == 'DOI'):\n",
    "            sl\n",
    "            tmp = datasets[['data_set_id', 'unique_identifier','title', 'description']].loc[datasets[column[By]].str.contains('|'.join(slist))]\n",
    "            tmp2 = publications[['publication_id', 'unique_identifier','title']].loc[publications[column[By]].str.contains('|'.join(slist))]\n",
    "            return tmp if tmp.shape[0]>0 else tmp2            \n",
    "    except Exception as e:\n",
    "        print(\"ERROR \"+e)\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import random\n",
    "from networkx.readwrite import json_graph\n",
    "import json\n",
    "\n",
    "def read_json_file(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        js_graph = json.load(f)\n",
    "    return json_graph.node_link_graph(js_graph)\n",
    "def generatePairs(firstnode, allDatasets):\n",
    "    return [(firstnode, i) for i in allDatasets]\n",
    "def cosine_similarity(g,pairs):\n",
    "    preds = []\n",
    "    for u,v in pairs:\n",
    "        common = len(list(nx.common_neighbors(G, u, v)))\n",
    "        s = common / math.sqrt(len(G[u])*len(G[v]))\n",
    "        preds.append((u,v,s))\n",
    "    return preds\n",
    "def getSimilarity(node, g, metric):\n",
    "    allDatasets = [i for i in g.nodes if(str(i).startswith('data_'))]\n",
    "    \n",
    "    if (node.startswith('data_')):\n",
    "        titleID = df_datasets.title_id[df_datasets.data_set_id == int(node.replace('data_', ''))]\n",
    "        filterSim = list(df_datasets.data_set_id[df_datasets.title_id == titleID.iloc[0]])\n",
    "        filterSim = ['data_'+str(i) for i in filterSim]\n",
    "\n",
    "        filtered = [i for i in allDatasets if i not in filterSim]\n",
    "    else:\n",
    "        filtered = allDatasets\n",
    "    pairs = generatePairs(node, filtered)\n",
    "\n",
    "    if metric == 'Jaccard':\n",
    "        preds = nx.jaccard_coefficient(g, pairs)\n",
    "    elif metric == 'Adamic-Adar':\n",
    "        preds = nx.adamic_adar_index(g, pairs)\n",
    "#     elif metric == 'Hopcroft':\n",
    "#         preds = nx.ra_index_soundarajan_hopcroft(g, pairs)\n",
    "    elif metric == 'Cosine':\n",
    "        return cosine_similarity(g, pairs)\n",
    "    else:\n",
    "        return []\n",
    "        \n",
    "    res = []\n",
    "    for u, v, p in preds:\n",
    "        if p > 0.0:\n",
    "            res.append((u,v,p))\n",
    "    return res\n",
    "def getRecommendations(i, G, metric = 'Jaccard'):\n",
    "    tmp = getSimilarity(i, G, metric)\n",
    "    print(\"Fetching dataset ID: %s \\nCalculating %s scores for %s/%s datasets\"%(i, metric, len(tmp),len(allDatasets)))\n",
    "    if len(tmp)>0:\n",
    "        df = pd.DataFrame(tmp, columns=['x', 'data_set_id', 'score']).sort_values(by=['score'], ascending=False)#.reset_index()\n",
    "        df.data_set_id = [int(i.replace('data_', '')) for i in df.data_set_id]\n",
    "\n",
    "        ids = df.data_set_id[:N]\n",
    "        ids = df_datasets[df_datasets.data_set_id.isin(ids)]\n",
    "        ids = ids.groupby('title_id').first().reset_index()\n",
    "\n",
    "        res = pd.merge(ids, df, how='inner', on='data_set_id').sort_values(by=['score'], ascending=False).reset_index()\n",
    "        return res[['data_set_id', 'score']].iloc[:10]\n",
    "\n",
    "G = read_json_file('data/network_v2.5_contract.json')\n",
    "\n",
    "# partition = community.best_partition(G)\n",
    "# for i in G.nodes:\n",
    "#     G.nodes[i]['community'] = partition.get(i)\n",
    "\n",
    "titles = [simplify_title(i) for i in datasets.title]\n",
    "\n",
    "df_titles = pd.DataFrame(set(titles)).reset_index()\n",
    "df_titles.columns = ['title_id','title_unique']\n",
    "df_titles.title_id = ['title_'+str(i) for i in df_titles.index]\n",
    "\n",
    "df_datasets = datasets.copy()\n",
    "df_datasets['title_unique'] = [simplify_title(i) for i in df_datasets.title]\n",
    "df_datasets = pd.merge(df_datasets, df_titles, on='title_unique', how='left')\n",
    "\n",
    "allDatasets = [i for i in G.nodes if(str(i).startswith('data'))]\n",
    "allPubs = [i for i in G.nodes if(str(i).startswith('pub'))]\n",
    "N=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "302b18e5128c46d4b5f796c09f190d19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Label(value='Press to generate dataset recommendations')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90d18f5a9c2f44e29ac3c7cb31684462",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Dropdown(description='Entity Type', index=1, options=('Publication Paper', 'Dataset', 'Keyword'â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e10719bae6a54f1199f7a30e8ef5cf1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "btn = widgets.Button(description = \"Recommend Me\")\n",
    "typeDD = widgets.Dropdown(\n",
    "    options = ['Publication Paper', 'Dataset', 'Keyword'],\n",
    "    value = 'Dataset',\n",
    "    description = 'Entity Type'\n",
    ")\n",
    "searchBar = widgets.Text(\n",
    "    value = '',\n",
    "    placeholder = 'Insert Keyword/ID'\n",
    ")\n",
    "typeDict = {\n",
    "    'Publication Paper': 'pub_',\n",
    "    'Dataset': 'data_',\n",
    "    'Keyword': ''\n",
    "}\n",
    "metric = widgets.Dropdown(\n",
    "    options = ['Jaccard', 'Cosine', 'Adamic-Adar'],\n",
    "    value = 'Jaccard'\n",
    ")\n",
    "btn_reset = widgets.Button(description = 'Reset')\n",
    "display(Label('Press to generate dataset recommendations'))\n",
    "# display(HBox([typeDD, searchBar,btn, btn_reset]))\n",
    "output = widgets.Output()\n",
    "\n",
    "@output.capture()\n",
    "def button_handler(btn):\n",
    "    if typeDD.value == 'Keyword':\n",
    "        print('Feature unavailable')\n",
    "        return\n",
    "    elif (typeDD.value == 'Dataset') & (searchBar.value == ''):\n",
    "        sampleDataset = random.sample(allDatasets, 1)\n",
    "    elif (typeDD.value == 'Publication Paper') & (searchBar.value == ''):\n",
    "        sampleDataset = random.sample(allPubs, 1)\n",
    "    else:\n",
    "        sampleDataset = [typeDict[typeDD.value]+searchBar.value]\n",
    "    for i in sampleDataset:\n",
    "        res = getRecommendations(i, G, metric.value)\n",
    "        res = pd.merge(res, datasets[['data_set_id', 'title', 'description']], how='left', on='data_set_id')\n",
    "        display(\",\".join(str(i) for i in res.data_set_id[:10]))\n",
    "        display(res)\n",
    "def clear_output(btn_reset):\n",
    "    out.clear_output()\n",
    "btn.on_click(button_handler)\n",
    "btn_reset.on_click(clear_output)\n",
    "\n",
    "display(HBox([typeDD, searchBar, metric, btn]))\n",
    "display(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS Base",
   "language": "python",
   "name": "ds_base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
